{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backprop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjin2364/mit-deep-learning/blob/main/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9LTqgrvF5Bm"
      },
      "source": [
        "import numpy as np              # numerical computing library\n",
        "import matplotlib.pyplot as plt # plotting library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGeEJshQGKJx"
      },
      "source": [
        "# create some random data\n",
        "np.random.seed(0) # set random seed for reproducibility\n",
        "Ndatapoints = 100\n",
        "x = np.random.randn(Ndatapoints,1)\n",
        "y = 0.5*x**2 + 0.2*np.random.randn(Ndatapoints,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-uSl_L4GMGc"
      },
      "source": [
        "# implement a neural net with forward and backward functions\n",
        "class Linear():\n",
        "  def __init__(self, N, M): # N is dimensionality of the input, M is dimensionality of the output\n",
        "    self.N = N\n",
        "    self.M = M\n",
        "    self.W = np.random.randn(M, N)\n",
        "    self.b = np.random.randn(M,1)\n",
        "    self.x_in = np.zeros((N,1))\n",
        "    self.x_out = np.zeros((M,1))\n",
        "  def forward(self, x):\n",
        "    self.x_in = x\n",
        "    self.x_out = np.matmul(self.W,self.x_in) + self.b\n",
        "    return self.x_out\n",
        "  def backward(self): # returns dLdx, x_in, dLdb\n",
        "    return self.W, self.x_in, np.eye(self.M)\n",
        "\n",
        "class Relu():\n",
        "  def __init__(self, dim):\n",
        "    self.x_in = np.zeros((dim,1))\n",
        "    self.x_out = np.zeros((dim,1))\n",
        "  def forward(self, x):\n",
        "    self.x_in = x\n",
        "    self.x_out = np.maximum(self.x_in,0)\n",
        "    return self.x_out\n",
        "  def backward(self): # returns dLdx\n",
        "    return np.diag((self.x_in>=0).astype(np.float32)[:,0])\n",
        "\n",
        "class Net():\n",
        "  def __init__(self, in_dim, hid_dim, out_dim, lr=0.0001):\n",
        "    self.l1 = Linear(in_dim, hid_dim)\n",
        "    self.r1 = Relu(hid_dim)\n",
        "    self.l2 = Linear(hid_dim, hid_dim)\n",
        "    self.r2 = Relu(hid_dim)\n",
        "    self.l3 = Linear(hid_dim, out_dim)\n",
        "\n",
        "    self.layers = [self.l1, self.r1, self.l2, self.r2, self.l3]\n",
        "\n",
        "    self.lr = lr # learning rate\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_in = x\n",
        "    for layer in self.layers:\n",
        "      x_out = layer.forward(x_in)\n",
        "      x_in = x_out.copy()\n",
        "    y = x_out\n",
        "    return y\n",
        "  \n",
        "  def backward(self, dLdout):\n",
        "\n",
        "    for layer in self.layers[::-1]:\n",
        "      \n",
        "      if isinstance(layer, Linear):\n",
        "\n",
        "        # compute gradients for the layer\n",
        "        doutdin, x_in, doutdb  = layer.backward()\n",
        "        dLdin = np.matmul(dLdout, doutdin)\n",
        "        dLdW = np.matmul(x_in, dLdout)\n",
        "        dLdb = np.matmul(dLdout, doutdb)\n",
        "\n",
        "        # take a gradient step\n",
        "        layer.W -= self.lr*np.transpose(dLdW)\n",
        "        #layer.b -= self.lr*np.transpose(dLdb)\n",
        "\n",
        "      elif isinstance(layer, Relu):\n",
        "        doutdin = layer.backward()\n",
        "        dLdin = np.matmul(dLdout, doutdin)\n",
        "      \n",
        "      else: print('unrecognized layer type')\n",
        "      \n",
        "      # move on to next layer\n",
        "      dLdout = np.copy(dLdin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABTC6srK7yZ8"
      },
      "source": [
        "# instantiate a net\n",
        "net = Net(1,128,1,0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wR7exVG7SYU"
      },
      "source": [
        "# plot the data\n",
        "def plot_data(x,y):\n",
        "  plt.plot(x, y, 'o', markersize=5)\n",
        "  plt.axis('equal')\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "\n",
        "# plot the mapping the net performs over some interval\n",
        "def plot_net(net):\n",
        "  x_grid = np.expand_dims(np.arange(np.floor(np.min(x)),np.ceil(np.max(x)),0.1), axis=1)\n",
        "  y_grid = np.zeros_like(x_grid)\n",
        "\n",
        "  for i in range(y_grid.shape[0]): # produce y = f(x) for each x\n",
        "    y_grid[[i]] = net.forward(x_grid[[i]])\n",
        "  plt.plot(x_grid, y_grid)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('f(x)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmXvIA8i3pup"
      },
      "source": [
        "# fit the net to the data, using backprop\n",
        "Nepochs = 10\n",
        "for b in range(Nepochs):\n",
        "  J = 0\n",
        "  for i in range(x.shape[0]):\n",
        "    y_pred = net.forward(x[[i]])\n",
        "    L = ((y_pred-y[i])**2) # mse loss\n",
        "    J += L[0,0]\n",
        "    dLdy_pred = 2*(y_pred-y[i])\n",
        "    net.backward(dLdy_pred)\n",
        "  if (b % 1)==0: print('Total loss: {:1.2f}'.format(J))\n",
        "\n",
        "# plot\n",
        "plot_data(x,y)\n",
        "plot_net(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Gc5n_dJ-GY"
      },
      "source": [
        "# additional exercises\n",
        "# 1. add weight decay or dropout to regularize the network\n",
        "# 2. rewrite the training using different sized batches to compute each gradient update\n",
        "# 3. define a new layer type (e.g., softmax, L2-norm, etc) and write the backprop update for it"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}