{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VQGAN+CLIP tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjin2364/mit-deep-learning/blob/main/VQGAN%2BCLIP_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "# Generate images from text prompts with VQGAN and CLIP\n",
        "\n",
        "Tutorial by Phillip Isola\n",
        "\n",
        "[CLIP paper](https://arxiv.org/abs/2103.00020)<br>\n",
        "[VQGAN paper](https://arxiv.org/abs/2012.09841)\n",
        "\n",
        "This tutorial is a simplified version of the colab linked here: https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ\n",
        "\n",
        "Contributors to original colab seem to be Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings), https://twitter.com/advadnoun, Eleiber#8347, Crimeacs#8222 (https://twitter.com/EarthML1), and Abulafia#3734.\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "MIT License included in the colab from which this was derived:\n",
        "\n",
        "Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA1PHoJrRiK9"
      },
      "source": [
        "# Clone CLIP and VQGAN code\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers.git\n",
        "\n",
        "# install some helpful packages\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install imageio-ffmpeg   \n",
        "!pip install einops\n",
        "\n",
        "# make a directory to save videos\n",
        "!mkdir steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhdWrSxQhwg"
      },
      "source": [
        "# Download VQGAN generator trained on ImageNet\n",
        "vqgan_model_name = \"vqgan_imagenet_f16_16384\"\n",
        "!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.yaml' #ImageNet 16384\n",
        "!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.ckpt' #ImageNet 16384"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "source": [
        "# @title Load libraries and variables\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.insert(1, '/content/taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import taming.modules \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# the following functions are for dealing with backprop through the discrete latent variables in VQGAN\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "# MakeCutouts returns a cutn number of patches, of size cut_size, cropped from the image \n",
        "#  its foward method is applied to.\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
        "            K.RandomPerspective(0.7,p=0.7),\n",
        "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
        "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        \n",
        "        for _ in range(self.cutn):\n",
        "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
        "            cutouts.append(cutout)\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tthw0YaispD"
      },
      "source": [
        "## Settings for this run:\n",
        "Modify the text in prompt to make an image that depicts that text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdlpRFL8UAlW"
      },
      "source": [
        "#@title Parameters\n",
        "#A Van Gogh painting of the Stata Center at MIT\n",
        "#\"A Barcelona tile mosaic of the Stata Center at MIT\"\n",
        "prompt = \"Mulan hugging Michael Phelps.\" #@param {type:\"string\"}\n",
        "width =  256#@param {type:\"number\"}\n",
        "height = 256#@param {type:\"number\"}\n",
        "seed = 7#@param {type:\"number\"}\n",
        "max_iterations = 700#@param {type:\"number\"}\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompt=prompt,\n",
        "    size=[width, height],\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{vqgan_model_name}.yaml',\n",
        "    vqgan_checkpoint=f'{vqgan_model_name}.ckpt',\n",
        "    lr=0.1,\n",
        "    cutn=32,\n",
        "    display_freq=50,\n",
        "    seed=seed,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX56bq4rEKIp"
      },
      "source": [
        "# setup gpu and random seed\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "# load models\n",
        "vqgan_model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "clip_model = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "## some setup for clip and vqgan\n",
        "\n",
        "# number of image crops to analyze the loss on\n",
        "cut_size = clip_model.visual.input_resolution\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn)\n",
        "\n",
        "# parameters of the vqgan latent code\n",
        "f = 2**(vqgan_model.decoder.num_resolutions - 1)\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "e_dim = vqgan_model.quantize.e_dim\n",
        "n_toks = vqgan_model.quantize.n_e\n",
        "z_min = vqgan_model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "z_max = vqgan_model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "## we will be optimizing over the latent code z that is the input to the VQGAN generator\n",
        "\n",
        "# we start with a random set of values for z\n",
        "one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "z = one_hot @ vqgan_model.quantize.embedding.weight\n",
        "z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
        "z = torch.rand_like(z)*2\n",
        "\n",
        "# then we set up the Adam optimizer to optimize over z\n",
        "z.requires_grad_(True)\n",
        "optim = torch.optim.Adam([z], lr=args.lr)\n",
        "\n",
        "# we will use normalize to preprocess the images before processing them with clip\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "# synthesize an image from latent code z using the vqgan generator\n",
        "def synth(z):\n",
        "    z_q = vector_quantize(z.movedim(1, 3), vqgan_model.quantize.embedding.weight).movedim(3, 1)\n",
        "    return clamp_with_grad(vqgan_model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "# clip embedding for the text prompt (this will serve as our target for the clip embedding of the generated image)\n",
        "text_embedding = clip_model.encode_text(clip.tokenize(args.prompt).to(device)).float()\n",
        "text_embedding_normed = F.normalize(text_embedding, dim=1)\n",
        "\n",
        "\n",
        "def optimize_z(optim,z):\n",
        "\n",
        "    for i in tqdm(range(max_iterations)):\n",
        "      optim.zero_grad()\n",
        "\n",
        "      # generate image from z vector\n",
        "      img = synth(z)\n",
        "      \n",
        "      # measure loss as similarity between clip embedding of the generated image and clip embedding of the text prompt\n",
        "      img_embedding = clip_model.encode_image(normalize(make_cutouts(img))).float()\n",
        "      img_embedding_normed = F.normalize(img_embedding, dim=1)\n",
        "      loss = img_embedding_normed.sub(text_embedding_normed).norm(dim=1).div(2).arcsin().pow(2).mean()\n",
        "      \n",
        "      # do a step of gradient descent on z\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      with torch.no_grad():\n",
        "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "      # write the output image to disk\n",
        "      img = np.array(img.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "      img = np.transpose(img, (1, 2, 0))\n",
        "      imageio.imwrite('./steps/' + str(i) + '.png', np.array(img))\n",
        "\n",
        "      # display it every now and then\n",
        "      if i % args.display_freq == 0:\n",
        "          print('iter {}'.format(i))\n",
        "          display.display(display.Image('./steps/' + str(i) + '.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLF2R2AZUfvf"
      },
      "source": [
        "optimize_z(optim,z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmK0k5zQeT5u"
      },
      "source": [
        "#@title Generate a video with the result\n",
        "\n",
        "init_frame = 1 #This is the frame where the video will start\n",
        "last_frame = max_iterations #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "min_fps = 10\n",
        "max_fps = 60\n",
        "\n",
        "total_frames = last_frame-init_frame\n",
        "\n",
        "length = 15 #Desired time of the video in seconds\n",
        "\n",
        "frames = []\n",
        "tqdm.write('Generating video...')\n",
        "for i in range(init_frame,last_frame): #\n",
        "    frames.append(Image.open(\"./steps/\"+ str(i) +'.png'))\n",
        "\n",
        "#fps = last_frame/10\n",
        "fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "p.wait()\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p16dUe8UYP1h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}